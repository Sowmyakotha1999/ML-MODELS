{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBVB61frq1Ce"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdPtMbNfsuKk"
      },
      "source": [
        "# // created synthetic dataset with following parameters\n",
        "Step 1: Created a synthetic dataset using \"sklearn.datasets.make_classification\". https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html . In this, we have set the following parameters for the dataset. n_samples=10000, n_features=20, n_informative=10, n_redundant=2, n_classes=2. That is, we are creating a dataset for binary classification with 10K samples, 20 features where there are only 10 informative features, 2 features are redundant ones (combinations of informative features) and the rest 20-10-2 = 8 features are just random noise.\n",
        "#A Brief Explanation:\n",
        "So, basically in the very first step I had included all the parameters or the values that has been asked to do so in the given problem/question statement.\n",
        "10K samples were considered with 20 features and 10 informative 2 redundant and 2 classes and imported the necessary libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QK9zIx6usz0B"
      },
      "outputs": [],
      "source": [
        "X, y = make_classification(n_samples=10000, n_features=20, n_informative=10, n_redundant=2, n_repeated=0, n_classes=2)  #I had included all the parameters or the values that has been asked to do so in the given problem/question statement.\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np #importing the numpy library as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTcxJ0cx8BQe"
      },
      "source": [
        "Next, we can define a function to perform 5-fold cross-validation on the dataset using the MLPClassifier with different settings of hidden_layer_sizes and activation functions:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCRTOXayYuXK"
      },
      "source": [
        "Step 2: Evaluate 5-fold cross validation performance on the synthetic dataset for the NNet classifier. \"sklearn.neural_network.MLPClassifier\". Here you can set the network structure using hidden_layer_sizes. For example, hidden_layer_sizes=(10,) means there is a single hidden layer with 10 units. Also, you can change the activation functions for the units using  the \"activation\" parameter. Experiment with different settings (1 hidden layer, 2 hidden layer with varying number of hidden nodes..there is no standard way to do this but just trial and error!) for which the performance seems reasonably good.\n",
        "**A Brief Explanation**\n",
        "So, for the second step, the problem statement has asked to do a 5-fold cross validation for the neural network classifier on the synthetic dataset which was considered in the initial step:1 , then using the hidder_layer functionality with a size of 1 layer on a count of 10 units.Then I tried to extent it with by trying 2 hidden layers for each of the 10 units."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRu5R2_w73Wu"
      },
      "outputs": [],
      "source": [
        "def evaluate_nnet(X, y, hidden_layer_sizes, activation):\n",
        "    crossfold = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, activation=activation, random_state=1,max_iter=1000)\n",
        "    scores = cross_val_score(crossfold, X, y, cv=5) # the scores has been stores in \"scores\"\n",
        "    return np.mean(scores) #return the scores accordingly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nitb8JyV9hkN",
        "outputId": "ece24f90-f6f7-44d5-d0cd-c6dea80fb537"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The actual Hidden Layer Sizes: (10,),  with the Activation: relu,  and the neural network Score: 0.9475\n",
            "The actual Hidden Layer Sizes: (10, 10),  with the Activation: relu,  and the neural network Score: 0.9526\n",
            "The actual Hidden Layer Sizes: (10,),  with the Activation: tanh,  and the neural network Score: 0.9509000000000001\n",
            "The actual Hidden Layer Sizes: (10, 10),  with the Activation: tanh,  and the neural network Score: 0.9593\n"
          ]
        }
      ],
      "source": [
        "\n",
        "settings = [\n",
        "    {\"hidden_layer_sizes\": (10,), \"activation\": \"relu\"}, # tried with different activation functions relu and tanh taking two hidden layers each with 10 units\n",
        "    {\"hidden_layer_sizes\": (10, 10), \"activation\": \"relu\"},\n",
        "    {\"hidden_layer_sizes\": (10,), \"activation\": \"tanh\"},\n",
        "    {\"hidden_layer_sizes\": (10,10), \"activation\": \"tanh\"},\n",
        "]\n",
        "\n",
        "# Evaluate each setting and print the results\n",
        "for setting in settings:\n",
        "    hidden_layer_sizes = setting[\"hidden_layer_sizes\"]\n",
        "    activation = setting[\"activation\"]\n",
        "    score = evaluate_nnet(X, y, hidden_layer_sizes, activation)\n",
        "    print(f\"The actual Hidden Layer Sizes: {hidden_layer_sizes},  with the Activation: {activation},  and the neural network Score: {score}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCgNnxomANrE"
      },
      "source": [
        "Step 3: Increase the number of noisy features in the data by decreasing n_informative in Step 1 and generate  2 other synthetic datasets with different number of n_informative features. Apply the evaluation in step 2 (no need to change the neural network architecture at this point but just use the one that gave you best performance with the first dataset). How did the Neural Network performance change as the number of noisy features in the dataset increased? You can report the average 5-fold cross validation scores to compare the performance.\n",
        "second synthetic dataset 1\n",
        "**A Breif Explanation :**\n",
        "So, in this step the problem statement requires to work on the number of noisy features. Now, I performed two different noisy features one with by decreasing the n_informative to 9 so that we could have the noisy feature as 9 and similarly for the other one I had decreased the n-Informative to 7 so with this we could have the noisy feature as 11 respectively.To perform this I had choosen two other synthetic datasets and applied the model accordingly.So, I had increased the noisy feature for this and let's see the results as follows below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VYWGAPayAWAk"
      },
      "outputs": [],
      "source": [
        "X1, y1 = make_classification(n_samples=10000, n_features=20, n_informative=9, n_redundant=2, n_repeated=0, n_classes=2)\n",
        "X2, y2 = make_classification(n_samples=10000, n_features=20, n_informative=7, n_redundant=2, n_repeated=0, n_classes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0Bju7slVAh4J"
      },
      "outputs": [],
      "source": [
        "X1, y1 = make_classification(n_samples=10000, n_features=20, n_informative=9, n_redundant=2, n_repeated=0, n_classes=2)\n",
        "def evaluate_nnet_dataset1(X1, y1, hidden_layer_sizes, activation):\n",
        "    crossfold = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, activation=activation, random_state=1,max_iter=1000)\n",
        "    scores = cross_val_score(crossfold, X1, y1, cv=5)\n",
        "    return np.mean(scores)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kMTPfKKqgOHN"
      },
      "outputs": [],
      "source": [
        "def evaluate_nnet_dataset2(X2, y2, hidden_layer_sizes, activation):\n",
        "    crossfold = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, activation=activation, random_state=1,max_iter=1000)\n",
        "    scores = cross_val_score(crossfold, X2, y2, cv=5)\n",
        "    return np.mean(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SZfnWheuAuMG",
        "outputId": "8ae814c6-6757-4bc2-e8ff-cf86eeec6772"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Hidden_Layer_Sizes: (10,), with Activation: relu, and the neural network Score: 0.9731\n",
            "The Hidden_Layer_Sizes: (10,), with Activation: relu, and the neural network Score: 0.9655999999999999\n",
            "The Hidden_Layer_Sizes: (10, 10), with Activation: relu, and the neural network Score: 0.9761000000000001\n",
            "The Hidden_Layer_Sizes: (10, 10), with Activation: relu, and the neural network Score: 0.9786999999999999\n"
          ]
        }
      ],
      "source": [
        "# Settings to try\n",
        "settings = [\n",
        "    {\"hidden_layer_sizes\": (10,), \"activation\": \"relu\"},\n",
        "    {\"hidden_layer_sizes\": (10, 10), \"activation\": \"relu\"}, # tried with different activation functions relu  taking two hidden layers each with 10 units\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "# here, Evaluated each setting and print the results accordingly.\n",
        "for setting in settings:\n",
        "    hidden_layer_sizes = setting[\"hidden_layer_sizes\"]\n",
        "    activation = setting[\"activation\"]\n",
        "    score_x1 = evaluate_nnet_dataset1(X1, y1, hidden_layer_sizes, activation)\n",
        "    print(f\"The Hidden_Layer_Sizes: {hidden_layer_sizes}, with Activation: {activation}, and the neural network Score: {score_x1}\")\n",
        "    score_x2 = evaluate_nnet_dataset1(X2, y2, hidden_layer_sizes, activation)\n",
        "    print(f\"The Hidden_Layer_Sizes: {hidden_layer_sizes}, with Activation: {activation}, and the neural network Score: {score_x2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mupXiSRAIiYV"
      },
      "source": [
        "and here it is, basically what I did observed from the above results is like, If the noise increases then accuracy will be decreased."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVpTZ2V5Lv46"
      },
      "source": [
        "using logistic regression\n",
        "Step 4: Use Logistic Regression with L1 penalty (also called LASSO) on the same data (as in step 3) and compare their performance with neural networks. Which approach performed better as the number of n_informative features were varied in the data?\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.htmlLinks to an external site.\n",
        "\n",
        "**Explanation**\n",
        "So, here in the final step of the given problem statement step:4, It has been asked to perfom the logistic regression with a penalty value of L1 and If in case the penalty was not mentioned initially, then I got to knew that the model with consider the penality as L2 which is the default one (just referred from the one of the blogs).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9n_47V7eMFM8",
        "outputId": "816cb8d0-b921-47cd-f9a0-4ac366d0bb78"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " the mean accuracy  with the Logistic_Regression_L1 penalty_(LASSO) for first_synthetic_dataset = 0.808\n",
            "  the mean accuracy  with the Logistic_Regression_L1 penalty_(LASSO) for second_synthetic_dataset = 0.802\n",
            "  the mean accuracy  with the Logistic_Regression_L1 penalty_(LASSO) for third_synthetic_dataset = 0.829\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "X4, y4 = make_classification(n_samples=10000, n_features=20, n_informative=10, n_redundant=2, n_repeated=0, n_classes=2)\n",
        "X5, y5 = make_classification(n_samples=10000, n_features=20, n_informative=9, n_redundant=2, n_repeated=0, n_classes=2)\n",
        "X6, y6 = make_classification(n_samples=10000, n_features=20, n_informative=7, n_redundant=2, n_repeated=0, n_classes=2)\n",
        "\n",
        "# Defined the Logistic Regression with L1 penalty (LASSO)\n",
        "lr_model = LogisticRegression(penalty='l1', solver='saga', max_iter=1000) #if the penalty wasn't given then the model would consider the default penalty if required.\n",
        "\n",
        "# here bascially, Evaluating the model using 5-fold cross-validation\n",
        "scores_x4 = cross_val_score(lr_model, X4, y4, cv=5)\n",
        "scores_x5 = cross_val_score(lr_model, X5, y5, cv=5)\n",
        "scores_x6 = cross_val_score(lr_model, X6, y6, cv=5)\n",
        "# Print the mean accuracy using the logistic regression\n",
        "print(f\" the mean accuracy  with the Logistic_Regression_L1 penalty_(LASSO) for first_synthetic_dataset = {scores_x4.mean():.3f}\")\n",
        "print(f\"  the mean accuracy  with the Logistic_Regression_L1 penalty_(LASSO) for second_synthetic_dataset = {scores_x5.mean():.3f}\")\n",
        "print(f\"  the mean accuracy  with the Logistic_Regression_L1 penalty_(LASSO) for third_synthetic_dataset = {scores_x6.mean():.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20xbNE-P7Mo6"
      },
      "source": [
        "compare logistic regression performance with neural networks model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BbKhrutmT3Tw",
        "outputId": "0e3d416e-bace-4ccf-ae46-79b1898011ff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "So, when the n_informative=6\n",
            "then the we have Logistic Regression accuracy: 0.810\n",
            "and then the we have Neural Network accuracy: 0.958\n",
            "\n",
            "So, when the n_informative=7\n",
            "then the we have Logistic Regression accuracy: 0.831\n",
            "and then the we have Neural Network accuracy: 0.912\n",
            "\n",
            "So, when the n_informative=8\n",
            "then the we have Logistic Regression accuracy: 0.720\n",
            "and then the we have Neural Network accuracy: 0.934\n",
            "\n",
            "So, when the n_informative=9\n",
            "then the we have Logistic Regression accuracy: 0.799\n",
            "and then the we have Neural Network accuracy: 0.951\n",
            "\n",
            "So, when the n_informative=10\n",
            "then the we have Logistic Regression accuracy: 0.816\n",
            "and then the we have Neural Network accuracy: 0.951\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate synthetic data with varying number of informative features\n",
        "for n_informative in range(6, 11):\n",
        "    X24, y24 = make_classification(n_samples=10000, n_features=20, n_informative=n_informative, random_state=42)\n",
        "\n",
        "    # here ,we could see the data has been splitted for botht the train dataset and test dataset\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X24, y24, test_size=0.2, random_state=42)\n",
        "\n",
        "    # here, logistic regression model with L1 penalty (LASSO) is trained\n",
        "    logistic_model = LogisticRegression(penalty='l1', solver='saga', max_iter=1000)\n",
        "    logistic_model.fit(X_train, y_train)\n",
        "    logistic_preds = logistic_model.predict(X_test)\n",
        "    logistic_acc = accuracy_score(y_test, logistic_preds)\n",
        "\n",
        "    # here, neural network model is trained\n",
        "    nn_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=10000)\n",
        "    nn_model.fit(X_train, y_train)\n",
        "    nn_preds = nn_model.predict(X_test)\n",
        "    nn_acc = accuracy_score(y_test, nn_preds)\n",
        "\n",
        "    # Print the results accordingly\n",
        "    print(f\"So, when the n_informative={n_informative}\")\n",
        "    print(f\"then the we have Logistic Regression accuracy: {logistic_acc:.3f}\") # this was the accuracy for the logistic regression\n",
        "    print(f\"and then the we have Neural Network accuracy: {nn_acc:.3f}\")  # this was the accuracy for the neural network\n",
        "    print(\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ez--kpMo6wlz"
      },
      "source": [
        "Which approach performed better as the number of n_informative features were varied in the data?\n",
        "ANS: After comnparing the accuracy of both models we can conclude that neuralnetworks model performs better and it is the better approach.\n",
        "**Comparision of two approaches:**\n",
        ">>So, I performed the logistic regression with penalty L1 on the same synthetic dataset.\n",
        ">>Then, I had the compared the performance of both the neural network classification with hidder_layers and the logistic regression with a penalty L1.\n",
        ">>with the comparision of both of them ,  I could see/ observed that the Neural network has the better performance and accuracy score for the considered synthetic dataset rather than the logistic regression."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}